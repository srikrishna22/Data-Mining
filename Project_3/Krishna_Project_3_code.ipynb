{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from yellowbrick.classifier import ConfusionMatrix, ROCAUC\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data and creating appropriate target labels\n",
    "\n",
    "X = pd.read_csv('./Desktop/MetisProjects/Project_3/Tanzania_train_X.csv')\n",
    "y = pd.read_csv('./Desktop/MetisProjects/Project_3/Tanzania_train_y.csv')\n",
    "df = X.merge(y,on='id')\n",
    "\n",
    "dummy_targets.columns = ['id', 'functional', 'functional_needs_repairs', 'non_functional']\n",
    "dummy_targets = dummy_targets[dummy_targets.columns[1:]]\n",
    "concat_train_data = pd.concat([X,dummy_targets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df[['longitude', 'latitude', 'construction_year']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the number of values in dummy columns while preserving most of the information\n",
    "def funder_wrangler(row):  \n",
    "    '''Keep top 5 values and set the rest to 'other'''\n",
    "\n",
    "    if row['funder']=='Government Of Tanzania':\n",
    "        return 'gov'\n",
    "    elif row['funder']=='Danida':\n",
    "        return 'danida'\n",
    "    elif row['funder']=='Hesawa':\n",
    "        return 'hesawa'\n",
    "    elif row['funder']=='Rwssp':\n",
    "        return 'rwssp'\n",
    "    elif row['funder']=='World Bank':\n",
    "        return 'world_bank'    \n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "df['funder'] = df.apply(lambda row: funder_wrangler(row), axis=1)\n",
    "\n",
    "\n",
    "#added status group_vals so that pivot table can be used to break down functionality of pumps based on funder\n",
    "vals_to_replace = {'functional':2, 'functional needs repair':1,\n",
    "                   'non functional':0}\n",
    "\n",
    "df['status_group_vals']  = df.status_group.replace(vals_to_replace)\n",
    "\n",
    "piv_table = pd.pivot_table(df,index=['funder','status_group'],\n",
    "                           values='status_group_vals', aggfunc='count')\n",
    "piv_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same idea as above reduce categories for one hot encoding\n",
    "\n",
    "def installer_wrangler(row):\n",
    "    '''Keep top 5 values and set the rest to 'other'''\n",
    "    if row['installer']=='DWE':\n",
    "        return 'dwe'\n",
    "    elif row['installer']=='Government':\n",
    "        return 'gov'\n",
    "    elif row['installer']=='RWE':\n",
    "        return 'rwe'\n",
    "    elif row['installer']=='Commu':\n",
    "        return 'commu'\n",
    "    elif row['installer']=='DANIDA':\n",
    "        return 'danida'\n",
    "    else:\n",
    "        return 'other'  \n",
    "\n",
    "df['installer'] = df.apply(lambda row: installer_wrangler(row), axis=1)\n",
    "\n",
    "df = df.drop('subvillage', axis=1)\n",
    "df.public_meeting = df.public_meeting.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheme_wrangler(row):\n",
    "    '''Keep top 5 values and set the rest to 'other'. '''\n",
    "    if row['scheme_management']=='VWC':\n",
    "        return 'vwc'\n",
    "    elif row['scheme_management']=='WUG':\n",
    "        return 'wug'\n",
    "    elif row['scheme_management']=='Water authority':\n",
    "        return 'wtr_auth'\n",
    "    elif row['scheme_management']=='WUA':\n",
    "        return 'wua'\n",
    "    elif row['scheme_management']=='Water Board':\n",
    "        return 'wtr_brd'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['scheme_management'] = df.apply(lambda row: scheme_wrangler(row), axis=1)\n",
    "\n",
    "#lot of categories with no clear top dogs. Probably safe to drop.\n",
    "df = df.drop('scheme_name', axis=1)\n",
    "df.permit = df.permit.fillna('Unknown')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = df.select_dtypes('object')\n",
    "str_cols.apply(lambda x: len(x.unique()))\n",
    "\n",
    "df.date_recorded = pd.to_datetime(df.date_recorded)\n",
    "df.date_recorded.describe()\n",
    "\n",
    "df.date_recorded = pd.datetime(2013, 12, 3) - pd.to_datetime(df.date_recorded)\n",
    "df.columns = ['days_since_recorded' if x=='date_recorded' else x for x in df.columns]\n",
    "df.days_since_recorded = df.days_since_recorded.astype('timedelta64[D]').astype(int)\n",
    "df.days_since_recorded.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the different basins to see if any stand out for being nonfunctional\n",
    "piv_table = pd.pivot_table(df, index=['basin', 'status_group'],\n",
    "                           values=['status_group_vals'], aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highly correlated with each other. I'll drop then for now.\n",
    "# They could be be worth including though, so might come back to them.\n",
    "df = df.drop(['region', 'lga', 'ward'], axis=1)\n",
    "\n",
    "\n",
    "df = df.drop('wpt_name', axis=1) #no relevant information\n",
    "df = df.drop('recorded_by', axis=1) # no relevant information\n",
    "df = df.drop(['extraction_type', 'extraction_type_group'], axis=1) # similar information\n",
    "df = df.drop('management', axis=1) # redundant information\n",
    "df = df.drop('management_group', axis=1) # redundant information\n",
    "df = df.drop('payment', axis = 1) # redundant information\n",
    "df = df.drop('quality_group', axis = 1) #redundant information\n",
    "df = df.drop('quantity_group', 1)\n",
    "df = df.drop('source', 1)\n",
    "df = df.drop(['gps_height', 'longitude', 'latitude', 'region_code', 'district_code','num_private', 'id'], axis=1)\n",
    "df = df.drop('status_group_vals', 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construction_wrangler(row):\n",
    "    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:\n",
    "        return '60s'\n",
    "    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:\n",
    "        return '70s'\n",
    "    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:\n",
    "        return '80s'\n",
    "    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:\n",
    "        return '90s'\n",
    "    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:\n",
    "        return '00s'\n",
    "    elif row['construction_year'] >= 2010:\n",
    "        return '10s'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "df['construction_year'] = df.apply(lambda row: construction_wrangler(row), axis=1)\n",
    "\n",
    "df.construction_year.value_counts()\n",
    "\n",
    "df.drop('status_group_vals', axis=1)\n",
    "\n",
    "df.to_csv('./Desktop/project3_ready_to_model.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Desktop/project3_ready_to_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = ['funder', 'installer', 'basin', 'public_meeting', 'scheme_management', 'permit',\n",
    "              'construction_year', 'extraction_type_class', 'payment_type', 'water_quality',\n",
    "              'quantity', 'source_type', 'source_class', 'waterpoint_type',\n",
    "             'waterpoint_type_group']\n",
    "\n",
    "df = pd.get_dummies(df, columns = dummy_cols)\n",
    "df.status_group=df.status_group.apply(lambda row: 2 if row == 'functional' else 1 if row == 'non functional' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('status_group', axis=1)\n",
    "y = df.status_group\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which classifier work best by default and take it from there\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred =clf.predict(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "mod = LogisticRegression()\n",
    "mod.fit(X_train, y_train)\n",
    "y_pred = mod.predict(X_test)\n",
    "mod.score(X_test, y_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "kNN.fit(X_train, y_train)\n",
    "y_pred = kNN.predict(X_test, y_test)\n",
    "kNN.score(X_test, y_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearSVC().get_params()\n",
    "\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "param_grid = {'clf__C':[0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__class_weight':[None, 'balanced']}\n",
    "\n",
    "estimator = GridSearchCV(estimator=pipe_svc,\n",
    "                         param_grid=param_grid)\n",
    "\n",
    "best_params = estimator.best_params_\n",
    "\n",
    "validation_accuracy = estimator.score(X_test, y_test)\n",
    "print('Validation accuracy: ', validation_accuracy)\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientBoostingClassifier().get_params()\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "param_grid = {'learning_rate': np.linspace(0.01, .1, 10),\n",
    "              'max_depth': [10, 15],\n",
    "              'min_samples_leaf':np.linspace(10,20, 10),\n",
    "              'max_features': [1.0],\n",
    "              'n_estimators': [100,200],\n",
    "              }\n",
    "\n",
    "estimator = GridSearchCV(gbm, param_grid=param_grid, n_jobs=-1)\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "best_params = estimator.best_params_\n",
    "print(best_params)\n",
    "\n",
    "test_accuracy = estimator.score(X_test, y_test)\n",
    "\n",
    "print('validation accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(learning_rate= 0.07, max_depth=14, min_samples_leaf=16, max_features=1.0, n_estimators = 100)\n",
    "\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "test_accuracy = gbm.score(X_test, y_test)\n",
    "\n",
    "print('Validation accuracy:', test_accuracy)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(gbm, classes=[0,1,2])\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof(outpath='./Desktop/Class_prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ROCAUC(gbm)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof(outpath='./Desktop/ROC.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=ConfusionMatrix(gbm)\n",
    "cm.score(X_test, y_test)\n",
    "for label in cm.ax.texts:\n",
    "    label.set_size(16)\n",
    "cm.poof('./Desktop/Confus.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "feature_importances=pd.DataFrame(columns=['feature','importance'])\n",
    "\n",
    "feature_importances['feature']= X_train.columns\n",
    "\n",
    "feature_importances['importance']=gbm.feature_importances_\n",
    "\n",
    "feature_importances= feature_importances.set_index('feature')\n",
    "\n",
    "feature_importances['importance'].sort_values(ascending = False).head(5).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnostics\n",
    "feature_importance = list(gbm.feature_importances_)\n",
    "column_names = list(X.columns)\n",
    "\n",
    "zipped = list(zip(column_names, feature_importance))\n",
    "\n",
    "sorted(zipped, key= lambda item:item[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.status_group.replace({1:'non-functional', 0:'needs_repairs', 2:'functional'}, inplace=True)\n",
    "df.population.describe()\n",
    "df.status_group[(df.days_since_recorded > 1400)].value_counts()\n",
    "df.status_group[(df.population > df.population.mean() + 2 * df.population.std())].value_counts() #2sds above and below\n",
    "df.status_group[(df.amount_tsh > df.amount_tsh.mean() + 2 * df.population.std())].value_counts() #2sds above and below\n",
    "\n",
    "df.status_group[df.quantity_dry == 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': [0.07],\n",
    "                        'max_depth': [14],\n",
    "                        'min_samples_leaf': [16],\n",
    "                        'max_features': [1.0],\n",
    "                        'n_estimators': [100]}                      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimator = GridSearchCV(estimator=GradientBoostingClassifier(),\n",
    "                         param_grid=best_params,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "estimator.fit(X, y_train)     \n",
    "\n",
    "test_accuracy = estimator.score(X_test, y_test)\n",
    "\n",
    "print('validation accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
